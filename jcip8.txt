[Chapter 8. Applying Thread Pools]

[[8.1. IMPLICIT COUPLINGS BETWEEN TASKS AND EXECUTION POLICIES]]
korabban azt mondtuk h Executor decouplolja a task submissiont az executiontol. valojaban nem minden task kompatibilis minden exec policyvel. taskok amelyek spec exec policyt igenyelnek
- dependent tasks: a legjobb taskok az independentek; amik nem fuggnek mas taskok timingjetol/resultjatol/side effectjeitol. ha indep taskokat execelunk thread poolban, akkor szabadon valthatjuk a pool sizet es configot, ez a perfen kivul masra nem lesz hatassal. de ha dep taskokat submittalunk thread poolba, az implicit constrainteket kreal az exec policyre, amikre figyelnunk kell ha nem akarunk liveness problemakat (ld. 8.1.1)
- tasks that exploit thread confinement: single thread executorok tobb garanciat vallalnak mint thread poolok. garantaljak h taskok nem lesznek conc execelve, ami miatt a task thread safetyje lazithato. az objk confinalhatoak a task threadbe, igy az abban a threadben valo futasra designolt taskok sync nelkul accessalhatjak oket, meg akkor is ha az objk nem threadsafek. ez implicit couplingot okoz a task es az exec policy kozott: a task executorja single threaded kell legyen; ha ahelyett thread poolt hasznalunk akkor a threadsafety elveszhet (a kovetelmeny valojaban nem ennyire szigoru; eleg biztositani h a taskok ne legyenek conc execelve, es legyen eleg sync h az egyik task altal okozott mem valtozasok visiblek legyenek a kov task szamara: newSingleThreadExecutor garantalja ezt)
- response-time-sensitive tasks: GUI appok resp time sensitivek, a user nem szereti ha a gombnyomas es a visual feedback kozott delay van. ha hosszan futo taskot submittolunk single thread executornak, vagy sok hosszan futo taskot submittolunk egy keves threades thread poolnak, az befolyasolhatja az executor altal managelt service respjat
- tasks that use ThreadLocal: ThreadLocal reven minden threadnek sajat private "verzioja" van a varbol. de az executorok szabadon reusolhatjak a threadeket: standard Executor implk lekaszalhatjak (reap) az idle threadeket ha low a demand, illetve uj threadeket addolhatnak ha high a demand; es replacelhetnek egy worker threadet ha egy task unchecked exct dob. ThreadLocalnak pool threadekben csakkor van ertelme, ha a task lifetimeja boundolja a var lifetimejat; ne hasznaljunk ThreadLocalt pool threadekben arra h taskok kozt valuekat kommunikaljunk

thread poolok akkor muk a legjobban ha a taskok homogenek es indepek. hosszu es rovid futasi ideju taskokat mixelve a pool "eltomodhet" ha nem eleg nagy; mas taskokon dependalo taskokat submittalva DL lephet fel hacsak a pool nem unbounded. szerencsere a tip nw-based server appok (webserver, mailserver, fileserver) requestjei alt teljesitik ezt

egyes taskok megkovetelnek v kizarnak spec exec policyt. mas taskokon dependalo taskok megkov h a thread pool eleg nagy legyen azert h a taskok sose legyenek queuzva/rejectalva; thread confinementet exploitolo taskok seq execet igenyelnek. ezeket dokumentalni kell h maintainerek kesobb nehogy inkompat exec policyra csereljek le

[[[8.1.1. Thread Starvation Deadlock]]]
ha mas taskokon dependalo taskokat thread poolban execelunk abbol DL lehet. single threaded executorban egy task ami egy masik taskot submitol uannak az executornak es var a resultja, az mindig DL lesz (a masodik task addig all a work queueban amig az elso completel, de az elso nem completel mert a masodik resultjara var). uez lehet nagyobb thread poolokban is ha minden thread olyan taskokat execel amelyek azert blokkolnak mert a work queuen levo egyeb taskokra varnak. ez az un thread starvation DL: akkor van ha egy pool task unbounded blocking waitet kezd egy olyan rscre/conditionra amit csak egy masik pool task actionja teljesithet (pl. a masik task return valueja v side effectje), kiveve ha a pool eleg nagy

class ThreadDeadlock {
  ExecutorService exec = Executors.newSingleThreadExecutor();
  class RenderPageTask implements Callable<String> {
    public String call() throws Exception {
	  Future<String> header, footer; header = exec.submit(new LoadFileTask("header.html")); footer = exec.submit(new LoadFileTask("footer.html")); String page = renderBody(); return header.get() + page + footer.get(); 
task (RenderPageTask) a (altala submittolt) subtask resultjara var, single threaded executorral ez mindig DL
egymas kozott barrierrel koordinalo taskoknal is fellephet ha a pool nem eleg nagy

ha nem indep taskokat submittolunk egy executornak akkor figyeljunk a thread starv DL lehetosegere, es dokumentaljuk a pool sizingre/configra von constrainteket
a thread pool size-jan kivul lehetnek megy egyeb implicit constraintek is, pl. ha 10 connra kepes JDBC conn poolt haszn es minden tasknak kell DB conn, az olyan mintha a thread poolban csak 10 thread lenne

[[[8.1.2. Long-running Tasks]]]
thread pooloknak akkor is resp problemai lehetnek ha a taskok hosszu ideig blokkolnak, meg ha DL nincs is. a pool "eltomodhet" az ilyen taskokkal, es meg a rovid futasi ideju taskok service timeja is megnohet. ha a pool size relative kicsi a hosszu futasi ideju taskok szamahoz kepest, akkor vegul minden pool thread ilyen taskokat fog futtatni, es resp-- lesz
segithet ha a taskok unbounded wait helyett timed rsc waitet haszn. legtobb blocking metodusnak a platform libekben van timed verzioja, pl. Thread.join(), BQ.put(), CountDownLatch.await(), Selector.select(). ha a wait timeoutol akkor a taskot failre markolhatjuk es abortalhatjuk v requeuolhatjuk kesobbi execre. igy minden task elobb-utobb completel (success v failure) a threadek pedig felszabadulnak. ha egy thread pool gyakran van tele blocked taskokkal azt jelezheti h a pool tul kicsi

[[8.2. SIZING THREAD POOLS]]
az idealis size a submittalando taskok tipusatol es a deployment sys jellemzoitol fugg. size ritkan hardcoded; configolhato v dinam van kiszamolva CPU szam alapjan
sizing nem egzakt tudomany, csak a "tul nagy" es "tul kicsi" kerulendo el. ha a pool tul nagy akkor a threadek keves CPU/mem rscert versenyeznek, magasabb mem usage, es valoszinubb rsc exhaustion. ha tul kicsi, az throughput-- mert a procok kihasznalatlanul allnak pedig lenne work
hany proc van a depl sysben? mennyi mem? taskok foleg szamolnak/IO/ketto kombinacioja? hasznalnak limitalt rsct pl JDBC connt? ha a taskjaink behav szerint tobb kategoriaba csoportosithatoak, akkor kategoriankent kulon thread poolt is hasznalhatunk

szamitasintenziv taskokra egy Ncpu (Runtime.availableProcessors()) procos sys opt kihasznalasa alt Ncpu+1 threades thread poollal (meg az ilyen taskoknal is lehet page fault v egyeb okbol pause, egy extra threadet hasznalva ilyenkor nem vesztunk CPU ciklusokat). IO-t v egyeb blocking opereket tartalmazo taskokhoz nagyobb pool kell mert nem minden thread schedulalhato mindig. task waiting time/compute time aranyt kell estimalni (nem kell pontosan, lehet pro-filinggel(?) v instrumentationnel). masik lehetoseg: tobb klf pool size mellett futtatni az appot benchmark loaddal, es megfigyelni a CPU kihasznaltsagot. az opt pool size: Ncpu * Ucpu * (1 + W/C), ahol Ucpu a kihasznaltsag (0 es 1 kozotti ertek)

CPU ciklusokon kivul mas rscok is szamitanak: mem, file handleok, socket handleok, DB connok. ezekre egyszerubb pool size constraintet szamolni: adjuk ossze h az egyes taskok mennyi rsct igenyelnek es osszuk el a total qtyvel, ez lesz a pool size felso hatara

ha taskok pooled rsct igenyelnek pl. DB conn, akkor a thread pool size es az rsc pool size affectalja egymast. ha minden tasknak kell conn, akkor a conn pool size limitalja a thread pool eff sizejat. hasonloan ha a connok consumerei kizarolag a pool taskok, akkor a thread pool size limitalja a conn pool sizeot

[[8.3. CONFIGURING THREADPOOLEXECUTOR]]
ThreadPoolExecutor a base impl az Executors-beli newCachedThreadPool(), newFixedThreadPool() es newScheduledThreadExecutor() factoryk altal visszaadott executorokhoz. flex, robust pool impl, jol customizalhato. ha a default exec policy nem megfelelo nekunk akkor peldanyosithatunk egy ThreadPoolExecutort a konstruktoraval es customizalhatjuk (exec policyk dokumentalva az Executors kodjaban)

[[[8.3.1. Thread Creation and Teardown]]]
TPE konstr paramok. core pool size a target size, probalja ezt megtartani meg ha eppen nincsenek is execelendo taskok (TPE krealaskor a core threadek nem startolnak el rogton, csak amikor a taskok submittolodnak; hacsak nem hivunk prestartAllCoreThreads()-t) es nem kreal ennel tobb threadet hacsak a work queue nincs tele. max pool size a felso hatar amennyi pool thread egyszerre aktiv lehet. keep-alive time ameddig egy thread idle maradhat, utana reaping candidate-te valik, es terminalodik ha a pool size tullepi a core sizet 

jo otletnek tunhet a core sizet 0-ra allitani, h a worker threadek idovel teardownolodjanak, es ne preventaljak a JVM exitet, de ez furcsa viselkedeshez vezethet az olyan thread pooloknal ahol nem SynchronousQueue a work queue (pl. newCachedThreadPool). ha a pool eleri a core sizeot, a TPE csakkor kreal uj threadet ha a work queue tele van. tehat egy olyan thread poolnal, ahol a core size zero es a work queuenak van vmennyi capacityje, a submittalt taskok nem fognak addig execelodni amig a queue meg nem telik, ami nem tul szerencses. allowCoreThreadTimeOut() segitsegevel kerheto h a pool threadek tudjanak timeoutolni; ha ezt 0 core size mellett kerjuk akkor lesz egy bounded poolunk bounded queueval, a threadek pedig teardownolodnak ha eppen nincs work

core pool size es keep-alive time tuneolasaval osztonozhetjuk a poolt h reapelje az idle threadeket es tegye oket elerhetove mas work szamara (tradeoff: ha kesobb ujra no a demand akkor a thread krealas latency++ jelent)

newFixedTreadPoolFactory a core es a max pool sizet is a requested sizera allitja amivel infinite timeout hatasat kelti; newCachedThreadPoolFactory a core sizeot 0-ra a max sizeot Integer.MAX_VALUE-ra allitja 1 perces timeouttal amivel infinit expandable pool hatasat kelti ami demand csokkenes eseten zsugorodik. ThreadPoolExecutor() konstr segitsegevel mas kombok is letrehozhatoak

[[[8.3.2. Managing Queued Tasks]]]
bounded thread poolok limitaljak a conc executolhato taskok szamat (single threaded executorok spec eset: garantaljak h nincs conc task execeles, thread safetyt a thread confinement garantalja)

6.1.2-ben lattuk h unbounded thread krealas instabot okozhat; ezert fix size thread poolt hasznaltunk es nem krealtunk minden requestre uj threadet. de ez nem teljes mo, az app heavy loadnal igy is kifuthat a rsckbol. ha az uj requestek erkezesi rateje nagyobb mint a rate amivel kezelni tudjuk oket akkor bequeueolodnak. thread poolnal a requestek egy Executor altal managelt Runnable queueban varnak, es nem threadkent; ami olcsobb de a rsc kimerules igy is elofordulhat ha a client gyorsabban kuldi a requesteket a servernek mint ahogy az kezelni tudja oket

requestek gyakran burstokben erkeznek meg ha az atlagos request rate stabil is. queuek segitsegevel ezt vmennyire kisimithatjuk, de ha a requestek tul gyorsan erkeznek, akkor throttlolni kell az erkezest, kulonben resp time++ ahogy a task queue no, es vegul kifutunk a membol (~ komm halokbeli flow controlhoz: bizonyos menny datat bufferelhetunk, de vegul meg kell kernunk a sendert h alljon le v eldobni az erkezo datat es remelni h majd ujrakuldik)

TPE-ben hasznalhatunk BQ-t az execre varo taskok tarolasara. task queueing harom modja: unbounded queue, bounded queue, sync handoff

newFixedThreadPool es newSingleThreadExecutornal a default az unbounded LinkedBQ. taskok queueolodnak ha minden worker thread busy, de a queue unbounded nohet ha a taskok gyorsabban erkeznek mint ahogy execelni tudnank oket

stabilabb rsc mgmt a bounded queue, pl. ArrayBQ v bounded LBQ vagy PriorityBQ. bounded queuek preventaljak a rsc kimerulest, de kerdes h mi lesz az uj taskokkal ha a queue tele van (ld. 8.3.3 saturation policyk). a queue es pool sizeot egyutt kell tuneolni. nagy queue + kis pool csokkentheti CPU/mem hasznalatot es ctx switchinget, de throughput constraintet okozhat

nagy unbounded pooloknal ki is lehet hagyni a queuet, es direktben atadni a taskokat a producerektol a worker threadeknek SynchronousQueue segitsegevel. ez valojaban nem queue hanem egy threadek kozti handoff mech. ahhoz h egy elemet rakhassunk a SQ-ra, egy masik threadnek mar varnia kell a handoff acceptalasra. ha nincs ilyen varo thread de a jelenlegi pool size < max akkor a TPE egy uj threadet kreal, egyebkent a task rejectalodik. ez a direkt handoff hatekonyabb mert a task rogton az execelo threadhez kerul, amelynek igy nem kell queuebol kifetchelnie. akkor jo valasztas ha a pool unbounded v ha a taskok rejectalasa elfogadhato. newCachedThreadPool SQ-t hasznal

FIFO queue (ABQ v LBQ) eseten a taskok az erkezi sorrendnek megfeleloen startolnak. PBQ segitsegevel lehet a taskokat prio szerint rendezni (prio def by natural order v Comparator)

newCachedThreadPool jo default az Executorhoz, jobb queue perf mint fixed size thread pool (ez a SQ hasznalatabol ered). fixed size thread pool akkor jo ha rsc mgmt okokbol limitalni akarjuk a conc taskok szamat, pl. server app ami vulnerable az overloadra

thread pool v work queue boundolasa csak indep taskoknal jo. dep taskoknal starv DL-t okozhat, helyette inkabb haszn unboundedet pl. newCachedThreadPool (v bounded pool + SQ + caller runs saturation policy is mukodhet)

[[[8.3.3. Saturation Policies]]]
akkor jatszik ha egy bounded work queue megtelik v ha taskot egy shutdownolt executornak submittolnak. TPE saturation policyje setRejectedExecutionHandler()-el allithato. AbortPolicy, CallerRunsPolicy, DiscardPolicy, DiscardOldestPolicy

default policy az abort, ilyenkor az execute() unchecked RejectedExecutionExceptiont dob; a hivo ezt catchelheti es kezelheti. discard policy csendben discardolja a submittolt taskot ha nem lehet execre bequeuezni. discard oldest policy azt a taskot discardolja amelyiket egyebkent a kovetkezokent execelnenk, es megprobalja helyette az uj taskot resubmitolni (PQ work queuenal ez a legmagasabb prioju taskot discardolja ami nem tul jo)

caller runs policy egyfajta throttlingot impl ami se nem discardol se nem dob exct; helyette megprobalja a task flowt lassitani azaltal h a work egy reszet visszapusholja a hivonak. nem pool threadben execeli a taskot hanem abban a threadben ami az execute()-ot hivja. ha a WebServer peldat modositanank bounded queuera es caller runs policyre, akkor ha az osszes pool thread foglalt lenne a work queue pedig tele, akkor a kov task a main threadben futna. mivel ez vmennyi idot vesz igenybe, a main thread egy darabig nem tud uj taskot submittolni, ezalatt a worker threadek kicsit catchupolhatnak. a main thread szinten nem hiv accept()-et ezalatt, ezert a bejovo requestek a TCP layerben gyulnek fel es nem az appban. ha az overload sokaig megmarad akkor vegul a TCP layer is ugy dont h discardolja a conn reqeket. azaz a server overload "kifele tolodik", a pool threadekbol a work queueba, az appba, a TCP layerbe es vegul a clientbe - more graceful degradation

saturation policy kivalasztasa v exec policy valtoztatasa az Executor krealasakor. pl. fixed size thread pool + caller runs
ThreadPoolExecutor executor = new ThreadPoolExecutor(N_THREADS, N_THREADS, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>(CAPACITY)); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.callerRunsPolicy()); 

nincs predef satur policy ami blokkolna az execute()-ot ha a work queue tele van. de egy Semaphore segitsegevel boundolhato a task injection rate. ilyenkor haszn unbounded queuet (nincs ertelme a task inj ratet es a queuet is boundolni) es allitsuk a sem boundjat (pool size + nr of queued tasks)-ra, mert a semaphore a conc execelt es az execre varo taskok szamat is boundolja
public class BoundedExecutor {
  private final Executor exec; private final Semaphore semaphore;
  public void submitTask(final Runnable command) throws InterruptedException {
    semaphore.acquire();
    try { exec.execute(new Runnable() {public void run() { try {command.run();} finally {semaphore.release();} }} ); } 
    catch(RejectedExecutionException e) {semaphore.release();} 

[[[8.3.4. Thread Factories]]]
thread pool thread factory segitsegevel tud threadet krealni. default thread factory nondaemon threadet kreal, spec config nelkul. thread factoryval a pool threadek configjat is customizalhatjuk 
custom thread factory hasznalata mellett tobb ok szol. pl. UncaughtExceptionHandlert akarunk def a pool threadek szamara, custom Thread classt akarunk peldanyositani (pl. ami debug loggingot csinal), pool threadek priojat akarjuk modifyolni (alt nem jo otlet, ld.10.3.1), be akarjuk allitani a daemon statust (ez se jo otlet ld. 7.4.2) v csak meaningful nevet akarunk adni a pool threadeknek h thread dumpot/error logot egyszerubb legyen olvasni

public class MyThreadFactory implements ThreadFactory { private final String poolName; public Thread newThread(Runnable runnable) { return new MyAppThread(runnable, poolName); 
MyAppThread-et peldanyosit, pool nevet atadja a konstrnak, igy az egyes poolok threadjei elkulonithetoek a thread dumpban/error logban. MyAppThread az appban mashol is hasznalhato igy minden thread kihasznalhatja a debugging featurejeiet
(MyAppThread kodja: beallithato a thread neve, custom UncaughtExceptionHandler ami loggerbe ir, stat a krealt/destroyolt threadek szamarol, optional debug msg thread krealaskor/terminalodaskor)

ha az appunk security polcyket hasznal (permissionok grantelese bizonyos codebasek szamara) akkor Executors.privilegedThreadFactory(). a pool threadek permissionjai, AccessControlContextje, contextClassLoaderje uaz lesz mint a privilegedThreadFactoryt krealo threade. egyebkent a pool threadek attol a clienttol oroklik a permissionoket ami epp a submit()/execute()-ot hivja amikor uj threadre van szukseg, ez pedig zavaro security exceket okozhat

[[[8.3.5. Customizing ThreadPoolExecutor After Construction]]]
TPE konstruktornak atadott legtobb param kesobb setterek segitsegevel modosithato (pl. core pool size, max pool size, keep-alive time, thread factory, rejected exec handler). ha az Executors factory metodusok vmelyikevel krealjuk az executort, akkor a settereket ugy tudjuk accessalni h TPE-ve castoljuk az eredmenyt
ExecutorService exec = Executors.newCachedThreadPool(); if(exec instanceof ThreadPoolExecutor) (ThreadPoolExecutor(exec).setCorePoolSize(10)); else throw new AssertionError("bad assumption");

Executors.unconfigurableExecutorService() bewrappel egy existing ExecutorServicet ugy h csak az ExecutorService metodusok lesznek exposolva, tehat nem lesz configolhato. newSingleThreadExecutor() egy ilyen wrappelt ExecutorServicet returnol es nem egy raw TPE-t. azert mert bar a single threaded executor ugy van impl mint egy thread pool egy darab threaddel, ugyanakkor garantalja h taskok nem lesznek conc execelve, es nem lenne jo ha vmilyen ellenorizetlen kod csak ugy megnovelne a pool sizet

[[8.4. EXTENDING THREADPOOLEXECUTOR]]
TPE-ben tobb hook van amit a subclassok overridolhatnak: beforExecute(), afterExecute(), terminated()

beforeExecute()/afterExecute() a taskot execelo threadbol hivodnak; logging/timing/monitoring/stat gathering adhato veluk hozza. afterExecute() akkor hivodik amikor a task completel; normal return a run()-bol v exc dobas (Error dobas eseten nem hivodik). ha beforeExecute() RTE-t dob akkor a task nem execelodik, az afterExecute() pedig nem hivodik meg. terminated() akkor hivodik amikor a thread pool befejezi a shutdown processt, minden task befejezodott es minden worker thread shutdownolt. hasznalhato az executor altal a lifecycle soran allokalt rsck releaselesere, notifra, loggingra, stat gathering finalizalasra

[[[8.4.1. Example: adding statistics to a thread pool]]]
public class TimingThreadPool extends ThreadPoolExecutor
custom thread pool, ami a fenti hookokkal loggingot/stat gatheringet ad hozza. task runtime mereshez a beforeExecute() rogziti es tarolja a start timeot h afterExecute() megtalalja. mivel az exec hookok a taskot execelo threadben hivodnak, egy beforeExecute() altal ThreadLocalba tett valuet az afterExecute() retrievelni tud

[[8.5. PARALLELIZING RECURSIVE ALGORITHMS]]
6.3-an page renderinget tobbszor atalakitottuk. eloszor seq volt, aztan ket thread volt de az img letoltesek meg mindig seq voltak, a vegso verzio minden img letoltest kulon taskkent kezelt. az olyan loopok amelyekben nontrivial szamitas v blocking IO van jo alanyok a parhuzamositasra, amennyiben az iteraciok fgtlek

ha olyan loopunk van ahol az iterek fgtlek es nem kell mindegyiket megvarni h tovabb lehessen haladni akkor egy Executorral a seq loop parhba alakithato
void processSeq(List<Element> elements) { for(Element e:elements) process(e);
void processInParallel(Executor exec, List<Element> elements) { for(final Element e: elements) { exec.execute(new Runnable() { public void run() { process(e);});
processInParralel()-bol a hivas gyorsabban returnol mint a processSeq()-bol mert mar akkor returnol amikor minden task bequeuezodik az Executorba, es nem varja meg amig mind completel. ha azt akarjuk h meg legyen varva amig minden submittolt task completel, akkor ExecutorService.invokeAll() es CompletionService-el retrievelni lehet az eredmenyeket mihelyt hferhetoek (ld. Renderer korabban)

seq loopok akkor jo alanyok parhra ha minden iter fgtl a tobbitol es az iterekben vegzett work eleg szignifikans ahhoz h megerje az uj task ktget

loop parhuzamositas rekurziv algokra is mukodhet; ezekben gyakran vannak seq loopok amik az elobbihez hasonloan parh atalakithatoak. az egyszerubb eset ha az iternek nincs szuksege az altala invokolt rekurziv iterek eredmenyere
public<T> void seqRecursive(List<Node> nodes, Collection<T> results) { for(Node<T> n: nodes) { results.add(n.compute()); seqRecursive(n.getChildren(), results);
public<T> void parallelRecursive(final Executor exec, List<Node<T>> nodes, final Collection<T> results) { for(final Node<T> n: nodes) { exec.execute(new Runnable() {public void run() {results.add(n.compute());} }}); parallelRecursive(exec, n.getChildren(), results);
seq tree melysegi bejarasa, kiszam minden nodeot es collba rakja a resultot. a parh verzio szinten melysegi bejaras de nem szamolja ki az eredmenyt amikor a node visited, hanem submittol egy taskot h az szamoljon
parallelRecursive() returnolesekor minden tree node visited (a traversalas tovabbra is seq csak a compute() hivasok parh) es minden node szamitasa be lett queuezva az Executornak. a parallelRecursive() hivoi megvarhatjak az osszes eredmenyt egy Executor segitsegevel
public<T> Collection<T> getParallelResults(List<Node<T>> nodes) throws InterruptedException {
  ExecutorService exec = Executors.newCachedThreadPool(); Queue<T> resultQueue = new ConcurrentLinkedQueue<T>();
  parallelRecursive(exec, nodes, resultQueue); exec.shutdown(); exec.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS); return resultQueue;

[[[8.5.1. Example: A Puzzle Framework]]]
egy jo pelda erre a technikara az olyan puzzleok megoldasa ahol egy init statebol egy goal stateba valo transformok sorozatat kell megtalalni (pl. sliding block puzzles)
hosszabb esettanulmany, nem irom le
